# MIPT course "Machine Learning". Basic Track. Fall 2021.

**Repository contains most interesting in my opinion task conditions and my solutions that were offered on basic track of the [course](https://github.com/girafe-ai/ml-mipt/tree/21f_basic) "Machine Learning" at MIPT.**

# Implemented tasks

+ ## [Decision Tree](https://github.com/grgera/ML-mipt-basic/blob/main/Solutions/assignment_04/tree.ipynb) 
In this assignment, you had to write your own Decision Tree, implementing the process of building, optimizing, and predicting yourself.

+ ## [NN from scratch](https://github.com/grgera/ML-mipt-basic/tree/main/Solutions/assignment_05)
Implementing all the basic blocks of the neural network in pure `NumPy`.
Modules (layers, loss functions, optimizers, etc.) are in the `modules1.ipynb`. Neural Network application and analysis of activation functions and optimization methods are in `main_notebook.ipynb`

+ ## [Lab 2: Dealing with overfitting](https://github.com/grgera/ML-mipt-basic/blob/main/Solutions/Lab2_part_2.ipynb)
Train a FC network, overfit it, try to deal with overfitting  by using regularization techniques (Dropout/Batchnorm/...).

+ ## [Lab 3: Poetry generation](https://github.com/grgera/ML-mipt-basic/blob/main/Solutions/Lab2_part_3.ipynb)
Generation of a poem using RNN and LSTM trained on the texts of A.S. Pushkin's "Eugene Onegin".

+ ## [Lab 4: Dogs classification](https://github.com/grgera/ML-mipt-basic/blob/main/Solutions/Lab2_part_4.ipynb)
Selection of the optimal network structure to solve the problem of classifying dog breeds. Dataset is represented by 50 classes of dog breeds.
My trained network gave 94% accuracy in a closed test.



